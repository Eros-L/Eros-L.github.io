---
title: 学习报告(week&nbsp;4)
date: 2018-11-16 10:53:00
description: Keep it SMPL&#58; Automatic Estimation of 3D Human Pose and Shape from a Single Image
categories:
 - weekly
tags: 
- note
---

## 学习报告(week&nbsp;4)

----------

### [Keep it SMPL&#58; Automatic Estimation of 3D Human Pose and Shape from a Single Image](https://arxiv.org/abs/1607.08128v1)

#### Getting Start

这篇文献介绍了一种新技术 **SMPLify**。通过使用 **SMPLify**，我们可以在不进行手工标注特征的情况下将一幅单纯 2D 的人体姿势图像转化为对应的 3D 模型。首先，作者使用了一个名为 **DeepCut** 的 CNN 来提取 2D 图像输入的特征关节点，但此时的关节点是缺乏深度信息的。作者接下来对上一步获得的 2D 关节点使用他们先前提出的 **SMPL** 模型来生成对应的 3D 模型，然后通过 Powell’s dogleg method(狗腿算法) 来优化 3D 模型的关节点，最后获得所需要的输出。 <br />

#### DeepCut

在正式介绍 **SMPLify** 的内容前，我想先简略地描述一下 **SMPLify** 所用到的两个重要的工具。DeepCut 是一个 CNN，它首先提取图片中的 body part candidates(人体部件候选区域)，然后令每一个候选区域对应一个关节点，令每一个关节点作为图中的一个节点。接下来，DeepCut 将属于同一个人的关节点归位一类，同时标记每一个节点属于哪一个人体部分。最后，分别将同一类中拥有不同标记的节点组合成一个人的姿态估计。 <br />

这种姿态估计的文章涉及到很多新的知识点，很难在一时半刻完全掌握 DeepCut 的内容，所有这里就暂时不进行过多的介绍了。在这篇文章中，我们只需要知道，DeepCut 可以提取出 2D 人像中的关节点，并对这些关节点计算出对应的自信度。实际上，目前已有名为 **Deepercut**，它貌似在效率与效果上都对 **DeepCut** 进行了改进，如果在 **SMPLify** 上应用 **Deepercut** 的话，或许能获得更好的效果。但考虑到我们项目的主要方向是 fashion style generator，在此就暂时不深究这方面的内容的(如果日后要寻求进一步的改善的话，**Deepercut** 技术或许是一个很好的 alternative)，我们只需要探讨如何把 **SMPLify** 应用到自己的项目中即可。 <br />

此处是一些相关的阅读材料：[DeepCut: Joint subset partition and labeling for multi person pose estimation](https://arxiv.org/abs/1511.06645)、[DeeperCut: A Deeper, Stronger, and Faster Multi-Person Pose Estimation Model](https://arxiv.org/abs/1605.03170)。 <br />

#### SMPL

**SMPL** 是一种基于参数化模型的人体建模方法，它由 Loper et al. 所提出。要彻底弄清楚 **SMPL** 的优势需要大量的背景知识，我们这里只需先了解它的一些基本信息。在 **SMPL** 中，一个 3D 模型可以被 β 与 θ 表示，其中，β 为 body shape(即人体的高矮胖瘦、头身比例等 10 个参数)，θ 为 body pose(即人体整体运动位姿和 24 个关节的相对角度，共 75 个参数)。 <br />

此处是一些相关的阅读材料：[SMPL: A Skinned Multi-Person Linear Model](http://files.is.tue.mpg.de/black/papers/SMPL2015.pdf)、[SMPL official website](http://smpl.is.tue.mpg.de)。 <br />

#### SMPLify

##### Introduction

**SMPLify** 在 2016 由 Bogo et al. 提出，它能将一张 2D 人像转化为对应的 3D 模型，并在包含众多复杂姿势的 Leeds Sports Pose Dataset 中取得了不错的成果，其官方网站为：[SMPLify official website](http://smplify.is.tuebingen.mpg.de)。 <br />

我们可以从一下的效果图中观察到，**SMPLify** 的确能在处理 2D 人像转化为对应的 3D 模型的问题上有优秀的表现，同时，它的最大优点在于不需要进行手动特征标注即可获得很好的转换效果(尽管如此，手动进行性别标注可以获得更好的效果)。 <br />

![](https://raw.githubusercontent.com/Eros-L/Eros-L.github.io/master/_posts/thesis/week4/smplify_example_result.png)

且外，文章也介绍了若干的 related works，结果是 **SMPLify** 在多个测试集下均能获得较好的结果，但我们无需对这些进行深入了解，所以就不再赘述了。 <br />

##### Method

首先，我们先通过一张图片来加深对 **SMPLify** 实现流程的初步认识： <br />

![](https://raw.githubusercontent.com/Eros-L/Eros-L.github.io/master/_posts/thesis/week4/smplify_example_result.png)

**SMPLify** 先接收到一张 2D 的人像图片，除了性别信息外，我们不需要再提供其他的额外信息。接下来，我们使用之前提及过的 **DeepCut** 网络来获得 2D 人像的节点信息。之后，我们使用 **SMPL** 来生成对应的 3D 模型，并通过 Powell’s dogleg method 来最小化生成模型的节点与 2D 人像节点的误差，从而获得最优的模型。 <br />

**SMPL** 人体模型可表示为 *M(β,θ,γ)* 的形式，其中，*β* 为 body shape，*θ* 为 body pose，*γ* 为 translation(事实上，我们可以从后文中的值，这里的 translation 即为 camera translation)。需要注意的是，**DeepCut** 所输出的节点数目与 **SMPL** 输出的节点数目略有不同，作者这里将两种方法中相近的节点联系在了一起。 <br />

##### Approximating Bodies with Capsules

为了避免 interpenetration(即人体模型出现不自然的姿势，人的肢体不正常地相交在了一起)，我们往往需要进行额外的计算，而计算人体表面的 interpenetration 的开销是非常大的，作者因此使用了 capsule(胶囊体) 来近似人体模型的不同部分。在这里，作者使用了回归的方式完成从 body shape 与 body pose 到 capsule 的转换。 <br />

##### Objective Function

这部分属于文章的重点内容，我们首先介绍 Powell’s dogleg method 中用到的 Error 函数： <br />

![](https://raw.githubusercontent.com/Eros-L/Eros-L.github.io/master/_posts/thesis/week4/error.png)

在这条公式中，*β* 与 *θ* 已经解释过了，*Jest* 代表从 **DeepCut** 获得的 2D 节点，*K* 代表摄像机参数，四个不同 *λ* 代表各个子误差的权重。

![](https://raw.githubusercontent.com/Eros-L/Eros-L.github.io/master/_posts/thesis/week4/joint-based_error.png)

对于 joint-based error term，我们可以将其理解成直接计算对应节点间的距离误差。*J*(*β*)<sub <i i /> /> 表示根据 body shape 所获得的 3D 骨骼节点位置的预测


















##### Discussion

其实关于 Configuration 的一些细节在上文也已经提到过了，这里就简略地概括一下这部分的内容： <br />
- **VGG** 并没有像 ILSVRC 2012 与 ILSVRC 2013 的获奖项目一样使用较大的卷积核来获得较大的感受野，取而代之的是用一系列的较小的卷积核来获得较大的感受野。比如，3 个 3x3 的卷积核能获得的感受野与 1 个 7x7 的卷积核获得的感受野一样，如果输入输出的通道数都是 C，而前者需要用到的参数数目为 27C^2+C，后者则需要 49C^2+C。
- 在作者看来，使用 1x1 的卷积核可以在不影响感受野的情况下增强决策函数的非线性。


#### Classification Framework

##### Training

**VGG** 的训练过程参考了 Krizhevsky 等人的研究成果，作者使用带动量 (momentum，通俗地讲，动量能积攒历史的梯度，若当前时刻梯度与历史梯度相近，则当前的趋势会被加强；否则，当前的趋势会被抑制。使用 momentum 能加快学习速度，并且避免因学习率过大导致的无法收敛) 的小批量梯度下降 (mini-batch gradient descent，它最大的优势在于能加快收敛并且避免达到局部最优，但在这里，使用 mini-batch 还为并行计算提供了可能性) 来优化多类别逻辑回归问题 (multinomial logistic regression)。其中，batch size 为 256，momentum 为 0.9，同时加入了 weight decay (L2 正则化，用以约束 weight，此处取值为 5e-4) 与 dropout (这里的 dropout ratio 为 0.5，即训练只有半数隐层单元的神经网络) 以避免过拟合的问题。学习率 (learning rate) 的初始值为 1e-2，作者使用了 adaptive 的方法，在交叉验证集的准确率停止提升时，学习率将缩小为原来的 1/10，避免无法收敛的现象。 <br />

整个训练耗时 370K iterations (74 epochs)，尽管 **VGG** 的参数数目和深度都比 Krizhevsky 等人的研究成果要大，但实际所需的 epoch 数却更小。这里，作者提出了对此的两个猜想：1) 更深的网络深度与更小的卷积核尺寸无形中为网络增加了正则化效果 (我懂这句话的意思，但是不懂原理 QAQ)；2) 在某些层中执行了预初始化。 <br />

对于上面提出的第二个猜想，作者实际上是执行了一个很聪明的操作，作者先用随机的初始参数来对深度最小的 A 网络进行训练，此时的训练速度无疑会比较慢。然后作者把训练好的 A 的参数用作其余网络的初始化参数 (由于不同网络的深度不一样，作者只在每个网络的前四个卷积层与全连接层上使用了 A 的参数进行初始化)。虽然不同网络的深度不一样，但是它们要完成的任务是一样的，因此，A 网络的参数很有可能与其他网络的参数相近，那么使用 A 的参数对其他网络进行初始化就很有可能能加快训练速度。**VGG** 用 mean = 0 且 variance = 1e-2 的正态分布来对 weight 随机初始化，并将所有 bias 的初始值设成 0，而这也是一个比较常规的初始化操作。作者也提到了在论文发表后，他们发现了可以使用 Glorot & Bengio 等人的随机初始化流程来代替他们之前使用的预初始化操作。作者并没有对此进行深入讨论，但在完成自己的项目时，Glorot & Bengio 的这个成果可能能给我提供不少帮助。 <br />

为了扩充训练集，作者首先将图片 rescale，并将 rescale 后的图片 crop 成 224x224 的输入图片。且外，作者还对图片进行了随机水平翻转与 RGB 颜色变换来进一步扩充数据集。**VGG** 首先对训练集的图像进行各项同性缩放 (isotropically-rescaled，可以理解成缩放的时候不对图片进行扭曲，与之相对的各向异性缩放将无视原图像长宽比进行缩放)，然后记缩放后的图像最小边为 S (在这里，我认为作者是将原图像缩放成正方形，然后再截取出 224x224 的部分作为卷积网络的输入)。如果最小边 S 等于 224，那么使用 224x224 的 bounding box 截取出来的图像将是原缩放图像本身；若最小边远大于 224，那么截取出来的图像则是原缩放图像的一小部分。 <br />

对于图片的缩放操作，作者采取了两种不同的方案。第一种是固定上文提到的 S 的值，分别让其等于 256 和 384 来进行两次训练。第二种是使用图像尺度抖动 (scale jittering，即令缩放得到的图像的最小边的值在 Smin 与 Smax 之间，考虑到实际输入的图片可能有不同的尺寸，在训练时输入不同尺寸的图片显然能提升网络的表现)。为了使网络的收敛加快，作者也用到了前面训练 single-scale model 时获得的参数来对 multi-scale model 进行微调 (fine-tuning，用其他模型训练好的参数来对自己的模型进行初始化，让初始参数处于较好的位置)。 <br />

##### Testing

论文考虑了两种预测方式，第一种是 multi-crop，第二种是 densely。所谓 multi-crop 即对缩放后获得的图像进行随机裁剪，然后通过网络对每一个裁剪出来的样本进行预测，最后求均值获得完整图像的预测值。而 densely 即不进行裁剪，直接将缩放后的图像作为网络的输入，同时，将网络中的全连接层改为卷积层。对比起 multi-crop，densely 避免了 multi-crop 存在的重复运算 (应该是不同的 crop 有重叠部分的意思) 的问题。且外，multi-crop 使用的是 zero-padding，而 densely 使用的是 neighbouring-padding (这样的操作能增大执行卷积时的感受野)。为了获得更好的效果，**VGG** 将原图和水平翻转后的图像的得分的均值作为最终的得分。在我看来，除了上面提及的在 testing 时的速度提升，把全连接层去掉是肯定可以提升网络 training 时的速度。虽然作者说他们认为 multi-crop 的额外运算量并不能确保获得更好的准确率，但是 **VGG** 最好的表现是结合 multi-crop 与 densely 而得到的。 <br />

##### Implementation Details

作者使用了 C++ Caffe toolbox 来运行他们的网络，这个 toolbox 能支持并行计算的功能 (近年来，用 py 貌似会主流一点)。作者提到了可以进行针对 batch 或针对 layer 的并行计算来提升速度。 <br />


#### Classification Experiments

作者的成果是基于 ILSVRC-2012 dataset 的，这里就不对这个数据集进行描述了。 <br />

##### Single-Scale Evaluation

根据 single test scale 的结果，我们可以发现深度和感受野的大小会对结果产生影响 (配置 C 比配置 B 要好，因为额外的 1x1 卷积层为 C 增加了深度；配置 D 比配置 C 好，因为 D 将 C 中的 1x1 卷积层替换为了 3x3 卷积层，这增加了卷积层的感受野)。其中比较值得关注的是 Q = 384 与 Q = 0.5(Smin+Smax) 的对比，前者是使用 fixed S 训练得到的，后者是使用 jittering 的 S 训练得到的。两者在测试时的输入都是 fixed Q = 384，而结果却表明了，使用 jittering 训练能明显提升网络表现。 <br />

![](https://raw.githubusercontent.com/Eros-L/Eros-L.github.io/master/_posts/thesis/week3/single_test_scale.png)

##### Multi-Scale Evaluation

在测试 fixed S 模型时，Q = {S-32, S, S+32}，在测试 jittering S 时，Q = {Smin, 0.5(Smin+Smax), Smax}。根据结果，我们能发现 jittering 的 testing 输入也能提升网络的表现。对于为何 jittering 的输入也能提升 fixed S 模型的表现，我想不到一个合理的解释。 <br />

![](https://raw.githubusercontent.com/Eros-L/Eros-L.github.io/master/_posts/thesis/week3/multi_test_scale.png)

##### Multi-Crop Evaluation

作者在初稿时貌似没有使用 multi-crop，而是出于效率考虑，使用了 densely。而实验结果表明，multi-crop 与 densely 的均值能带来最好的表现。作者假设结果的提升是来自两种方法使用了不同的 padding 方式。 <br />

![](https://raw.githubusercontent.com/Eros-L/Eros-L.github.io/master/_posts/thesis/week3/mult_crop_scale.png)

##### ConvNet Fusion

作者只是比较暴力地对不同网络的分类概率求了平均值，事实证明，这样也能提升网络的表现。 <br />

![](https://raw.githubusercontent.com/Eros-L/Eros-L.github.io/master/_posts/thesis/week3/ConvNet_fusion.png)

##### Comparison

这个数据只用参考一下即可。数据表明，有较大深度的 **VGG** 和 **GoogLeNet** 能获得更好的效果，其中 top-1 error 即预测值等于实际类标才视为正确，top-5 error 即概率最大的五个预测值包含正确类标则视为正确。 <br />

![](https://raw.githubusercontent.com/Eros-L/Eros-L.github.io/master/_posts/thesis/week3/comparison.png)


#### Localization

上文介绍的主要是使用 **VGG** 处理分类问题，在这里也补充说明一下用 **VGG** 处理定位问题的情况。这里有两种关于边界框预测的方案，一种是在所有类间共享的单类回归，另一种是针对特定类的每类回归。对于前一种输出的维度为 4x1，而在后一种则是 4x1000。除了最后的边界预测层外，前面使用的是配置 D 的卷积网络。 <br />

##### Training

由于时间限制，**VGG** 并没有使用 jittering S 的训练方式。且外，localization 网络的前 15 层使用了配置 D 的结果进行 fine-tuning，而最后一层则使用了随机初始化。 <br />

##### Testing

作者考虑了两种 testing 的方案。第一种只考虑不同的网络在验证集上的表现。网络的输入为图像的中心裁切，且只考虑 ground truth class 的输出。第二种则是使用 densly 的方法，先将获得的 1000 个预测边框中空间相近的边框进行合并，然后再通过这些边框在 classification 中的评分进行排序，获得最终输出。作者并没有使用 Sermanet 等人的 multiple pooling offsets technique 来提升预测边框的空间分辨率以进一步提升网络的表现。 <br />

##### Settings Comparison

在这里，如果预测的边界框与真实边界框的重叠率大于 0.5，则被认为是正确的。我们可以留意到，PCR 的表现明显比 SCR 要好，且外，对所用层使用 fine-tuning 也能获得更好的效果。 <br />

![](https://raw.githubusercontent.com/Eros-L/Eros-L.github.io/master/_posts/thesis/week3/localization_setting.png)

##### Fully-Fledged Evaluation

这里使用了之前得到的最好的 setting (即 PCR 加 fine-tuning)，然后我们可以发现，jittering Q 与 fusion 同样能提升网络在 localization 问题的表现。我估计使用 jittering S 与综合 multi-scale 和 densely 能进一步提升效果。 <br />

![](https://raw.githubusercontent.com/Eros-L/Eros-L.github.io/master/_posts/thesis/week3/fully-fledged_evaluation.png)


#### Generalization of Very Deep Features

在本章，**VGG** 将作为其他更小数据集的特征提取器，而结果也表明了 **VGG** 在其他数据集上的泛化能力很好。作者把 **VGG** 的最后一个全连接层去除，用倒数第二个全连接层的 4096 维的激活作为图像特征，且在整个过程中不对 pre-trained 网络进行修改。作者在训练时用到的策略与 classification 问题中的基本一致，不同的是这里对不同输入尺寸的网络进行了集成学习 (这里用到的是 stacking，对于这个问题，我觉得 boosting 也是一种可行的方案)。若干的实验数据表明，**VGG** 在不同的数据集上都能获得很好的效果。 <br />


----------

### Conclusion

**VGG** 是基于较大的训练集而训练得到的，而且它的参数数目最多可达到 144M 个，用四个 NVIDIA Titan Black GPU 来并行计算两到三周才能得到一个网络。因此，在开发自己的项目时，对 **VGG** 进行 fine-tuning 是不太实际的。但是，这篇文献中使用到了大量的训练 CNN 的技巧，而这些技巧都是很值得学习的。即便没有要修改 **VGG** 的打算，加深对它的了解也能让我在日后更好地把它应用到自己的项目中。 <br />





[它的官网](http://smpl.is.tue.mpg.de)
